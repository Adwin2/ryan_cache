// æµ‹è¯•å®¢æˆ·ç«¯ ï¼š ./bin/cache_client
package main

import (
	"fmt"
	"log"
	"sync"
	"sync/atomic"
	"time"

	"tdd-learning/distributed"
)

func main() {
	// åˆ›å»ºåˆ†å¸ƒå¼å®¢æˆ·ç«¯ - å¯ç”¨æ™ºèƒ½å¥åº·æ£€æŸ¥å’Œè´Ÿè½½å‡è¡¡
	config := distributed.ClientConfig{
		Nodes: []string{
			"localhost:8001",
			"localhost:8002",
			"localhost:8003",
		},
		Timeout:    5 * time.Second,
		RetryCount: 3,

		// æ–°å¢ï¼šæ™ºèƒ½èŠ‚ç‚¹ç®¡ç†é…ç½®
		HealthCheckEnabled:    true,
		HealthCheckInterval:   10 * time.Second, // 10ç§’æ£€æŸ¥ä¸€æ¬¡
		FailureThreshold:      2,                // 2æ¬¡å¤±è´¥åæ ‡è®°ä¸ºä¸å¥åº·
		RecoveryCheckInterval: 15 * time.Second, // 15ç§’æ£€æŸ¥æ¢å¤
	}

	client := distributed.NewDistributedClient(config)
	defer client.Close() // ä¼˜é›…å…³é—­ï¼Œåœæ­¢å¥åº·æ£€æŸ¥åç¨‹

	fmt.Println("ğŸ§ª æ™ºèƒ½åˆ†å¸ƒå¼ç¼“å­˜å®¢æˆ·ç«¯æµ‹è¯•")
	fmt.Println("================================")
	fmt.Println("âœ¨ æ–°ç‰¹æ€§ï¼šæ™ºèƒ½å¥åº·æ£€æŸ¥ + è‡ªåŠ¨æ•…éšœè½¬ç§» + è´Ÿè½½å‡è¡¡")
	fmt.Println()

	// ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨å’Œåˆå§‹å¥åº·æ£€æŸ¥
	fmt.Println("â³ ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨å’Œå¥åº·æ£€æŸ¥...")
	time.Sleep(5 * time.Second)
	
	// 1. æ™ºèƒ½èŠ‚ç‚¹çŠ¶æ€æ£€æŸ¥
	fmt.Println("1. ğŸ” æ™ºèƒ½èŠ‚ç‚¹çŠ¶æ€æ£€æŸ¥:")
	nodeStatus := client.GetNodeStatus()
	if nodeStatus == nil {
		fmt.Println("   âš ï¸  å¥åº·æ£€æŸ¥æœªå¯ç”¨æˆ–èŠ‚ç‚¹ç®¡ç†å™¨æœªåˆå§‹åŒ–")
	} else {
		fmt.Println("   ğŸ“Š è¯¦ç»†èŠ‚ç‚¹çŠ¶æ€:")
		healthyCount := 0
		for node, status := range nodeStatus {
			healthIcon := "âŒ"
			if status.IsHealthy {
				healthIcon = "âœ…"
				healthyCount++
			}
			fmt.Printf("   %s èŠ‚ç‚¹ %s:\n", healthIcon, node)
			fmt.Printf("      å¥åº·çŠ¶æ€: %v\n", status.IsHealthy)
			fmt.Printf("      å¤±è´¥æ¬¡æ•°: %d\n", status.FailureCount)
			fmt.Printf("      æœ€åæ£€æŸ¥: %v\n", status.LastCheckTime.Format("15:04:05"))
			if !status.LastSuccessTime.IsZero() {
				fmt.Printf("      æœ€åæˆåŠŸ: %v\n", status.LastSuccessTime.Format("15:04:05"))
			}
			if !status.LastFailureTime.IsZero() {
				fmt.Printf("      æœ€åå¤±è´¥: %v\n", status.LastFailureTime.Format("15:04:05"))
			}
			fmt.Println()
		}
		fmt.Printf("   ï¿½ å¥åº·èŠ‚ç‚¹: %d/%d\n", healthyCount, len(nodeStatus))
	}

	// ä¼ ç»Ÿå¥åº·æ£€æŸ¥å¯¹æ¯”
	fmt.Println("\n   ğŸ”„ ä¼ ç»Ÿå¥åº·æ£€æŸ¥å¯¹æ¯”:")
	if healthStatus, err := client.CheckHealth(); err != nil {
		log.Printf("   âŒ æ£€æŸ¥å¥åº·çŠ¶æ€å¤±è´¥: %v", err)
	} else {
		for node, healthy := range healthStatus {
			status := "âŒ ä¸å¥åº·"
			if healthy {
				status = "âœ… å¥åº·"
			}
			fmt.Printf("   èŠ‚ç‚¹ %s: %s\n", node, status)
		}
	}
	
	// 2. è·å–é›†ç¾¤ä¿¡æ¯
	fmt.Println("\n2. ğŸŒ è·å–é›†ç¾¤ä¿¡æ¯:")
	if clusterInfo, err := client.GetClusterInfo(); err != nil {
		log.Printf("âŒ è·å–é›†ç¾¤ä¿¡æ¯å¤±è´¥: %v", err)
	} else {
		fmt.Printf("   é›†ç¾¤ä¿¡æ¯: %+v\n", clusterInfo)
	}
	
	// 3. æ™ºèƒ½è´Ÿè½½å‡è¡¡æµ‹è¯•
	fmt.Println("\n3. âš–ï¸  æ™ºèƒ½è´Ÿè½½å‡è¡¡æµ‹è¯•:")
	testLoadBalancing(client)

	// 4. æµ‹è¯•åŸºæœ¬ç¼“å­˜æ“ä½œ
	fmt.Println("\n4. ğŸ“ æµ‹è¯•åŸºæœ¬ç¼“å­˜æ“ä½œ:")
	
	// æµ‹è¯•æ•°æ®
	testData := map[string]string{
		"user:1001":    "å¼ ä¸‰",
		"user:1002":    "æå››",
		"user:1003":    "ç‹äº”",
		"product:2001": "iPhone 15",
		"product:2002": "MacBook Pro",
		"order:3001":   "è®¢å•è¯¦æƒ…1",
		"order:3002":   "è®¢å•è¯¦æƒ…2",
		"session:abc":  "ç”¨æˆ·ä¼šè¯æ•°æ®",
	}
	
	// è®¾ç½®æ•°æ®
	fmt.Println("   è®¾ç½®æ•°æ®:")
	for key, value := range testData {
		if err := client.Set(key, value); err != nil {
			fmt.Printf("     âŒ è®¾ç½® %s å¤±è´¥: %v\n", key, err)
		} else {
			fmt.Printf("     âœ… è®¾ç½® %s = %s\n", key, value)
		}
	}
	
	// è·å–æ•°æ®
	fmt.Println("\n   è·å–æ•°æ®:")
	for key, expectedValue := range testData {
		if value, found, err := client.Get(key); err != nil {
			fmt.Printf("     âŒ è·å– %s å¤±è´¥: %v\n", key, err)
		} else if !found {
			fmt.Printf("     âš ï¸  %s æœªæ‰¾åˆ°\n", key)
		} else if value != expectedValue {
			fmt.Printf("     âŒ %s å€¼ä¸åŒ¹é…: æœŸæœ›=%s, å®é™…=%s\n", key, expectedValue, value)
		} else {
			fmt.Printf("     âœ… è·å– %s = %s\n", key, value)
		}
	}
	
	// 5. æµ‹è¯•æ‰¹é‡æ“ä½œ
	fmt.Println("\n5. ğŸ“¦ æµ‹è¯•æ‰¹é‡æ“ä½œ:")

	// æ‰¹é‡è®¾ç½®
	batchData := map[string]string{
		"batch:1": "æ‰¹é‡æ•°æ®1",
		"batch:2": "æ‰¹é‡æ•°æ®2",
		"batch:3": "æ‰¹é‡æ•°æ®3",
	}

	if err := client.BatchSet(batchData); err != nil {
		fmt.Printf("   âŒ æ‰¹é‡è®¾ç½®å¤±è´¥: %v\n", err)
	} else {
		fmt.Printf("   âœ… æ‰¹é‡è®¾ç½®æˆåŠŸ: %d ä¸ªé”®\n", len(batchData))
	}

	// æ‰¹é‡è·å–
	keys := []string{"batch:1", "batch:2", "batch:3"}
	if result, err := client.BatchGet(keys); err != nil {
		fmt.Printf("   âŒ æ‰¹é‡è·å–å¤±è´¥: %v\n", err)
	} else {
		fmt.Printf("   âœ… æ‰¹é‡è·å–æˆåŠŸ: %d ä¸ªé”®\n", len(result))
		for key, value := range result {
			fmt.Printf("     %s = %s\n", key, value)
		}
	}

	// 6. æµ‹è¯•åˆ é™¤æ“ä½œ
	fmt.Println("\n6. ğŸ—‘ï¸  æµ‹è¯•åˆ é™¤æ“ä½œ:")
	deleteKeys := []string{"user:1001", "product:2001", "batch:1"}

	for _, key := range deleteKeys {
		if err := client.Delete(key); err != nil {
			fmt.Printf("   âŒ åˆ é™¤ %s å¤±è´¥: %v\n", key, err)
		} else {
			fmt.Printf("   âœ… åˆ é™¤ %s æˆåŠŸ\n", key)
		}
	}

	// éªŒè¯åˆ é™¤
	fmt.Println("   éªŒè¯åˆ é™¤:")
	for _, key := range deleteKeys {
		if value, found, err := client.Get(key); err != nil {
			fmt.Printf("     âŒ éªŒè¯ %s å¤±è´¥: %v\n", key, err)
		} else if found {
			fmt.Printf("     âŒ %s ä»ç„¶å­˜åœ¨: %s\n", key, value)
		} else {
			fmt.Printf("     âœ… %s å·²è¢«åˆ é™¤\n", key)
		}
	}

	// 7. æ•…éšœè½¬ç§»æµ‹è¯•
	fmt.Println("\n7. ğŸ”„ æ•…éšœè½¬ç§»æµ‹è¯•:")
	testFailover(client)

	// 8. è·å–ç»Ÿè®¡ä¿¡æ¯
	fmt.Println("\n8. ğŸ“Š è·å–ç»Ÿè®¡ä¿¡æ¯:")
	if stats, err := client.GetStats(); err != nil {
		fmt.Printf("   âŒ è·å–ç»Ÿè®¡å¤±è´¥: %v\n", err)
	} else {
		fmt.Printf("   ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯: %+v\n", stats)
	}

	// 9. æ€§èƒ½æµ‹è¯•
	fmt.Println("\n9. âš¡ æ€§èƒ½æµ‹è¯•:")
	performanceTest(client)

	// 10. å¹¶å‘æ€§èƒ½æµ‹è¯•
	fmt.Println("\n10. ğŸš€ å¹¶å‘æ€§èƒ½æµ‹è¯•:")
	concurrentPerformanceTest(client)

	// 11. è¿æ¥æ± æ•ˆæœéªŒè¯
	fmt.Println("\n11. ğŸ”— è¿æ¥æ± æ•ˆæœéªŒè¯:")
	connectionPoolTest(client)
	
	fmt.Println("\nâœ… æ™ºèƒ½åˆ†å¸ƒå¼ç¼“å­˜æµ‹è¯•å®Œæˆï¼")
	fmt.Println("\nğŸ¯ æ–°ç‰¹æ€§éªŒè¯ç»“æœ:")
	fmt.Println("   âœ… æ™ºèƒ½å¥åº·æ£€æŸ¥ï¼šè‡ªåŠ¨ç›‘æ§èŠ‚ç‚¹çŠ¶æ€")
	fmt.Println("   âœ… æ•…éšœæ£€æµ‹ï¼šè‡ªåŠ¨è¯†åˆ«å’Œæ ‡è®°æ•…éšœèŠ‚ç‚¹")
	fmt.Println("   âœ… æ™ºèƒ½è´Ÿè½½å‡è¡¡ï¼šä¼˜å…ˆä½¿ç”¨å¥åº·èŠ‚ç‚¹")
	fmt.Println("   âœ… è‡ªåŠ¨æ•…éšœè½¬ç§»ï¼šæ•…éšœèŠ‚ç‚¹è‡ªåŠ¨åˆ‡æ¢")
	fmt.Println("   âœ… èŠ‚ç‚¹æ¢å¤ï¼šæ•…éšœèŠ‚ç‚¹æ¢å¤åè‡ªåŠ¨é‡æ–°ä½¿ç”¨")
	fmt.Println("   âœ… æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘è¯·æ±‚å¤±è´¥å’Œé‡è¯•")
	fmt.Println()
	fmt.Println("ğŸ’¡ ä¼ ç»Ÿç‰¹æ€§ä¿æŒ:")
	fmt.Println("   - æ•°æ®æ ¹æ®ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•åˆ†å¸ƒåœ¨ä¸åŒèŠ‚ç‚¹")
	fmt.Println("   - å®¢æˆ·ç«¯å¯ä»¥ä»ä»»æ„èŠ‚ç‚¹è®¿é—®ä»»æ„æ•°æ®")
	fmt.Println("   - è¯·æ±‚ä¼šè‡ªåŠ¨è½¬å‘åˆ°æ­£ç¡®çš„å­˜å‚¨èŠ‚ç‚¹")
	fmt.Println("   - æ”¯æŒæ‰¹é‡æ“ä½œå’Œæ€§èƒ½ä¼˜åŒ–")
	fmt.Println()
	fmt.Println("ğŸš€ ä¼ä¸šçº§ç‰¹æ€§:")
	fmt.Println("   - é›¶é…ç½®åŠ¨æ€èŠ‚ç‚¹ç®¡ç†")
	fmt.Println("   - ç”Ÿäº§ç¯å¢ƒå°±ç»ªçš„å¯é æ€§")
	fmt.Println("   - å®Œæ•´çš„ç›‘æ§å’ŒçŠ¶æ€æŸ¥è¯¢")
	fmt.Println("   - ä¼˜é›…çš„èµ„æºç®¡ç†å’Œå…³é—­")
}

// performanceTest æ€§èƒ½æµ‹è¯•
func performanceTest(client *distributed.DistributedClient) {
	fmt.Println("   æ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
	
	// æµ‹è¯•å‚æ•°
	numOperations := 1000
	
	// å†™æ€§èƒ½æµ‹è¯•
	start := time.Now()
	for i := 0; i < numOperations; i++ {
		key := fmt.Sprintf("perf:write:%d", i)
		value := fmt.Sprintf("value_%d", i)
		if err := client.Set(key, value); err != nil {
			fmt.Printf("     âš ï¸ å†™æ“ä½œå¤±è´¥: %v\n", err)
			break
		}
	}
	writeDuration := time.Since(start)
	writeOPS := float64(numOperations) / writeDuration.Seconds()
	
	// è¯»æ€§èƒ½æµ‹è¯•
	start = time.Now()
	for i := 0; i < numOperations; i++ {
		key := fmt.Sprintf("perf:write:%d", i)
		if _, _, err := client.Get(key); err != nil {
			fmt.Printf("     âš ï¸ è¯»æ“ä½œå¤±è´¥: %v\n", err)
			break
		}
	}
	readDuration := time.Since(start)
	readOPS := float64(numOperations) / readDuration.Seconds()
	
	fmt.Printf("   ğŸ“Š æ€§èƒ½ç»“æœ:\n")
	fmt.Printf("     å†™æ“ä½œ: %.2f ops/s (%d æ“ä½œï¼Œè€—æ—¶ %v)\n", writeOPS, numOperations, writeDuration)
	fmt.Printf("     è¯»æ“ä½œ: %.2f ops/s (%d æ“ä½œï¼Œè€—æ—¶ %v)\n", readOPS, numOperations, readDuration)
	
	// æ¸…ç†æ€§èƒ½æµ‹è¯•æ•°æ®
	for i := 0; i < numOperations; i++ {
		key := fmt.Sprintf("perf:write:%d", i)
		client.Delete(key)
	}
}

// testLoadBalancing æµ‹è¯•æ™ºèƒ½è´Ÿè½½å‡è¡¡
func testLoadBalancing(client *distributed.DistributedClient) {
	fmt.Println("   ğŸ”„ æ‰§è¡Œè´Ÿè½½å‡è¡¡æµ‹è¯•...")

	// å‘é€å¤šä¸ªè¯·æ±‚ï¼Œè§‚å¯Ÿè´Ÿè½½åˆ†å¸ƒ
	testKeys := []string{
		"lb:test:1", "lb:test:2", "lb:test:3", "lb:test:4", "lb:test:5",
		"lb:test:6", "lb:test:7", "lb:test:8", "lb:test:9", "lb:test:10",
	}

	fmt.Println("   ğŸ“¤ å‘é€æµ‹è¯•è¯·æ±‚...")
	successCount := 0
	for i, key := range testKeys {
		value := fmt.Sprintf("load_balance_value_%d", i)
		if err := client.Set(key, value); err != nil {
			fmt.Printf("     âš ï¸ è®¾ç½® %s å¤±è´¥: %v\n", key, err)
		} else {
			successCount++
		}
	}

	fmt.Printf("   ğŸ“Š è¯·æ±‚ç»“æœ: %d/%d æˆåŠŸ\n", successCount, len(testKeys))

	// éªŒè¯æ•°æ®
	fmt.Println("   ğŸ“¥ éªŒè¯æ•°æ®...")
	retrieveCount := 0
	for _, key := range testKeys {
		if _, found, err := client.Get(key); err != nil {
			fmt.Printf("     âš ï¸ è·å– %s å¤±è´¥: %v\n", key, err)
		} else if found {
			retrieveCount++
		}
	}

	fmt.Printf("   ğŸ“Š éªŒè¯ç»“æœ: %d/%d æ‰¾åˆ°\n", retrieveCount, len(testKeys))

	// æ˜¾ç¤ºå½“å‰èŠ‚ç‚¹çŠ¶æ€
	fmt.Println("   ğŸ“ˆ å½“å‰èŠ‚ç‚¹çŠ¶æ€:")
	nodeStatus := client.GetNodeStatus()
	for node, status := range nodeStatus {
		healthIcon := "âœ…"
		if !status.IsHealthy {
			healthIcon = "âŒ"
		}
		fmt.Printf("     %s %s (å¤±è´¥: %d)\n", healthIcon, node, status.FailureCount)
	}

	// æ¸…ç†æµ‹è¯•æ•°æ®
	for _, key := range testKeys {
		client.Delete(key)
	}
}

// testFailover æµ‹è¯•æ•…éšœè½¬ç§»
func testFailover(client *distributed.DistributedClient) {
	fmt.Println("   ğŸ”§ æ¨¡æ‹Ÿæ•…éšœè½¬ç§»åœºæ™¯...")

	// è®¾ç½®ä¸€äº›æµ‹è¯•æ•°æ®
	testData := map[string]string{
		"failover:1": "æ•°æ®1",
		"failover:2": "æ•°æ®2",
		"failover:3": "æ•°æ®3",
	}

	fmt.Println("   ğŸ“¤ è®¾ç½®æµ‹è¯•æ•°æ®...")
	for key, value := range testData {
		if err := client.Set(key, value); err != nil {
			fmt.Printf("     âš ï¸ è®¾ç½® %s å¤±è´¥: %v\n", key, err)
		}
	}

	// æ˜¾ç¤ºè®¾ç½®åçš„èŠ‚ç‚¹çŠ¶æ€
	fmt.Println("   ğŸ“Š è®¾ç½®åèŠ‚ç‚¹çŠ¶æ€:")
	nodeStatus := client.GetNodeStatus()
	if nodeStatus != nil {
		healthyNodes := 0
		for node, status := range nodeStatus {
			if status.IsHealthy {
				healthyNodes++
			}
			fmt.Printf("     èŠ‚ç‚¹ %s: å¥åº·=%v, å¤±è´¥=%d\n",
				node, status.IsHealthy, status.FailureCount)
		}
		fmt.Printf("   ğŸ’š å¥åº·èŠ‚ç‚¹æ•°: %d\n", healthyNodes)
	}

	// å°è¯•è®¿é—®æ•°æ®ï¼Œæµ‹è¯•å®¢æˆ·ç«¯çš„æ™ºèƒ½è·¯ç”±
	fmt.Println("   ğŸ“¥ éªŒè¯æ•°æ®è®¿é—®...")
	accessCount := 0
	for key := range testData {
		if _, found, err := client.Get(key); err != nil {
			fmt.Printf("     âš ï¸ è®¿é—® %s å¤±è´¥: %v\n", key, err)
		} else if found {
			accessCount++
		}
	}

	fmt.Printf("   ğŸ“Š è®¿é—®ç»“æœ: %d/%d æˆåŠŸ\n", accessCount, len(testData))

	// æç¤ºç”¨æˆ·å¯ä»¥æ‰‹åŠ¨æµ‹è¯•æ•…éšœè½¬ç§»
	fmt.Println("   ğŸ’¡ æ•…éšœè½¬ç§»æµ‹è¯•æç¤º:")
	fmt.Println("     - å¯ä»¥æ‰‹åŠ¨åœæ­¢ä¸€ä¸ªèŠ‚ç‚¹æ¥æµ‹è¯•æ•…éšœè½¬ç§»")
	fmt.Println("     - å®¢æˆ·ç«¯ä¼šè‡ªåŠ¨æ£€æµ‹æ•…éšœå¹¶åˆ‡æ¢åˆ°å¥åº·èŠ‚ç‚¹")
	fmt.Println("     - é‡å¯èŠ‚ç‚¹åä¼šè‡ªåŠ¨æ¢å¤å¹¶é‡æ–°åŠ å…¥è´Ÿè½½å‡è¡¡")

	// æ¸…ç†æµ‹è¯•æ•°æ®
	for key := range testData {
		client.Delete(key)
	}
}

// concurrentPerformanceTest å¹¶å‘æ€§èƒ½æµ‹è¯•
func concurrentPerformanceTest(client *distributed.DistributedClient) {
	fmt.Println("   ğŸ”„ æ‰§è¡Œå¹¶å‘æ€§èƒ½æµ‹è¯•...")

	// æµ‹è¯•å‚æ•°
	numOperations := 1000
	concurrencyLevels := []int{1, 5, 10, 20}

	for _, concurrency := range concurrencyLevels {
		fmt.Printf("\n   ğŸ“Š å¹¶å‘åº¦ %d æµ‹è¯•:\n", concurrency)

		// å¹¶å‘å†™æµ‹è¯•
		testConcurrentWrites(client, numOperations, concurrency)

		// å¹¶å‘è¯»æµ‹è¯•
		testConcurrentReads(client, numOperations, concurrency)

		// æ··åˆè¯»å†™æµ‹è¯•
		testMixedOperations(client, numOperations, concurrency)
	}
}

// testConcurrentWrites æµ‹è¯•å¹¶å‘å†™æ“ä½œ
func testConcurrentWrites(client *distributed.DistributedClient, total, concurrency int) {
	var wg sync.WaitGroup
	operationsPerWorker := total / concurrency

	start := time.Now()

	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			for j := 0; j < operationsPerWorker; j++ {
				key := fmt.Sprintf("concurrent:write:%d:%d", workerID, j)
				value := fmt.Sprintf("value_%d_%d", workerID, j)
				client.Set(key, value)
			}
		}(i)
	}

	wg.Wait()
	duration := time.Since(start)
	ops := float64(total) / duration.Seconds()

	fmt.Printf("     âœï¸  å¹¶å‘å†™: %.2f ops/s (%d æ“ä½œï¼Œ%d å¹¶å‘ï¼Œè€—æ—¶ %v)\n",
		ops, total, concurrency, duration)

	// æ¸…ç†æ•°æ®
	for i := 0; i < concurrency; i++ {
		for j := 0; j < operationsPerWorker; j++ {
			key := fmt.Sprintf("concurrent:write:%d:%d", i, j)
			client.Delete(key)
		}
	}
}

// testConcurrentReads æµ‹è¯•å¹¶å‘è¯»æ“ä½œ
func testConcurrentReads(client *distributed.DistributedClient, total, concurrency int) {
	operationsPerWorker := total / concurrency

	// å…ˆå‡†å¤‡æµ‹è¯•æ•°æ®
	for i := 0; i < concurrency; i++ {
		for j := 0; j < operationsPerWorker; j++ {
			key := fmt.Sprintf("concurrent:read:%d:%d", i, j)
			value := fmt.Sprintf("read_value_%d_%d", i, j)
			client.Set(key, value)
		}
	}

	var wg sync.WaitGroup
	start := time.Now()

	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			for j := 0; j < operationsPerWorker; j++ {
				key := fmt.Sprintf("concurrent:read:%d:%d", workerID, j)
				client.Get(key)
			}
		}(i)
	}

	wg.Wait()
	duration := time.Since(start)
	ops := float64(total) / duration.Seconds()

	fmt.Printf("     ğŸ“– å¹¶å‘è¯»: %.2f ops/s (%d æ“ä½œï¼Œ%d å¹¶å‘ï¼Œè€—æ—¶ %v)\n",
		ops, total, concurrency, duration)

	// æ¸…ç†æ•°æ®
	for i := 0; i < concurrency; i++ {
		for j := 0; j < operationsPerWorker; j++ {
			key := fmt.Sprintf("concurrent:read:%d:%d", i, j)
			client.Delete(key)
		}
	}
}

// testMixedOperations æµ‹è¯•æ··åˆè¯»å†™æ“ä½œ
func testMixedOperations(client *distributed.DistributedClient, total, concurrency int) {
	operationsPerWorker := total / concurrency
	var wg sync.WaitGroup

	start := time.Now()

	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			for j := 0; j < operationsPerWorker; j++ {
				key := fmt.Sprintf("concurrent:mixed:%d:%d", workerID, j)

				// 70%è¯»æ“ä½œï¼Œ30%å†™æ“ä½œ
				if j%10 < 7 {
					// è¯»æ“ä½œ
					client.Get(key)
				} else {
					// å†™æ“ä½œ
					value := fmt.Sprintf("mixed_value_%d_%d", workerID, j)
					client.Set(key, value)
				}
			}
		}(i)
	}

	wg.Wait()
	duration := time.Since(start)
	ops := float64(total) / duration.Seconds()

	fmt.Printf("     ğŸ”„ æ··åˆè¯»å†™: %.2f ops/s (%d æ“ä½œï¼Œ%d å¹¶å‘ï¼Œè€—æ—¶ %v)\n",
		ops, total, concurrency, duration)

	// æ¸…ç†æ•°æ®
	for i := 0; i < concurrency; i++ {
		for j := 0; j < operationsPerWorker; j++ {
			key := fmt.Sprintf("concurrent:mixed:%d:%d", i, j)
			client.Delete(key)
		}
	}
}

// connectionPoolTest è¿æ¥æ± æ•ˆæœéªŒè¯æµ‹è¯•
func connectionPoolTest(client *distributed.DistributedClient) {
	fmt.Println("   ğŸ”— éªŒè¯è¿æ¥æ± ä¼˜åŒ–æ•ˆæœ...")

	// æµ‹è¯•1: è¿ç»­è¯·æ±‚å»¶è¿Ÿæµ‹è¯•
	fmt.Println("\n   ğŸ“Š è¿ç»­è¯·æ±‚å»¶è¿Ÿåˆ†æ:")
	testConnectionReuse(client)

	// æµ‹è¯•2: çªå‘å¹¶å‘æµ‹è¯•
	fmt.Println("\n   ğŸ“Š çªå‘å¹¶å‘æµ‹è¯•:")
	testBurstConcurrency(client)

	// æµ‹è¯•3: é•¿æ—¶é—´è¿è¡Œæµ‹è¯•
	fmt.Println("\n   ğŸ“Š é•¿æ—¶é—´è¿è¡Œæµ‹è¯•:")
	testLongRunning(client)
}

// testConnectionReuse æµ‹è¯•è¿æ¥å¤ç”¨æ•ˆæœ
func testConnectionReuse(client *distributed.DistributedClient) {
	numRequests := 100
	latencies := make([]time.Duration, numRequests)

	// é¢„çƒ­ä¸€æ¬¡è¯·æ±‚ï¼Œå»ºç«‹è¿æ¥
	client.Set("warmup", "warmup_value")
	client.Delete("warmup")

	// æµ‹è¯•è¿ç»­è¯·æ±‚çš„å»¶è¿Ÿ
	for i := 0; i < numRequests; i++ {
		key := fmt.Sprintf("latency_test_%d", i)
		value := fmt.Sprintf("value_%d", i)

		start := time.Now()
		client.Set(key, value)
		latencies[i] = time.Since(start)

		client.Delete(key) // ç«‹å³æ¸…ç†
	}

	// åˆ†æå»¶è¿Ÿåˆ†å¸ƒ
	var total time.Duration
	var min, max time.Duration = latencies[0], latencies[0]

	for _, latency := range latencies {
		total += latency
		if latency < min {
			min = latency
		}
		if latency > max {
			max = latency
		}
	}

	avg := total / time.Duration(numRequests)

	fmt.Printf("     å¹³å‡å»¶è¿Ÿ: %v\n", avg)
	fmt.Printf("     æœ€å°å»¶è¿Ÿ: %v\n", min)
	fmt.Printf("     æœ€å¤§å»¶è¿Ÿ: %v\n", max)
	fmt.Printf("     å»¶è¿Ÿæ¯”å€¼: %.2fx (æœ€å¤§/æœ€å°)\n", float64(max)/float64(min))

	// åˆ†æå‰10æ¬¡å’Œå10æ¬¡çš„å»¶è¿Ÿå·®å¼‚
	var firstTen, lastTen time.Duration
	for i := 0; i < 10; i++ {
		firstTen += latencies[i]
		lastTen += latencies[numRequests-10+i]
	}

	firstTenAvg := firstTen / 10
	lastTenAvg := lastTen / 10

	fmt.Printf("     å‰10æ¬¡å¹³å‡: %v\n", firstTenAvg)
	fmt.Printf("     å10æ¬¡å¹³å‡: %v\n", lastTenAvg)

	if firstTenAvg > lastTenAvg {
		improvement := float64(firstTenAvg-lastTenAvg) / float64(firstTenAvg) * 100
		fmt.Printf("     ğŸš€ è¿æ¥å¤ç”¨æ•ˆæœ: å»¶è¿Ÿé™ä½ %.1f%%\n", improvement)
	} else {
		fmt.Printf("     ğŸ“Š è¿æ¥å¤ç”¨æ•ˆæœ: å»¶è¿Ÿç›¸å¯¹ç¨³å®š\n")
	}
}

// testBurstConcurrency æµ‹è¯•çªå‘å¹¶å‘åœºæ™¯
func testBurstConcurrency(client *distributed.DistributedClient) {
	burstSizes := []int{5, 10, 20, 50}

	for _, burstSize := range burstSizes {
		var wg sync.WaitGroup
		start := time.Now()

		// çªå‘å¹¶å‘è¯·æ±‚
		for i := 0; i < burstSize; i++ {
			wg.Add(1)
			go func(id int) {
				defer wg.Done()
				key := fmt.Sprintf("burst_%d_%d", burstSize, id)
				value := fmt.Sprintf("burst_value_%d", id)
				client.Set(key, value)
				client.Get(key)
				client.Delete(key)
			}(i)
		}

		wg.Wait()
		duration := time.Since(start)

		fmt.Printf("     çªå‘ %d å¹¶å‘: æ€»è€—æ—¶ %v, å¹³å‡ %v/è¯·æ±‚\n",
			burstSize, duration, duration/time.Duration(burstSize))
	}
}

// testLongRunning æµ‹è¯•é•¿æ—¶é—´è¿è¡Œåœºæ™¯
func testLongRunning(client *distributed.DistributedClient) {
	fmt.Println("     æ‰§è¡Œ30ç§’é•¿æ—¶é—´è¿è¡Œæµ‹è¯•...")

	duration := 30 * time.Second
	concurrency := 5

	var wg sync.WaitGroup
	var totalOps int64
	var totalErrors int64

	start := time.Now()
	stopChan := make(chan struct{})

	// å¯åŠ¨å®šæ—¶å™¨
	go func() {
		time.Sleep(duration)
		close(stopChan)
	}()

	// å¯åŠ¨å·¥ä½œåç¨‹
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			ops := 0
			errors := 0

			for {
				select {
				case <-stopChan:
					atomic.AddInt64(&totalOps, int64(ops))
					atomic.AddInt64(&totalErrors, int64(errors))
					return
				default:
					key := fmt.Sprintf("longrun_%d_%d", workerID, ops)
					value := fmt.Sprintf("value_%d", ops)

					if err := client.Set(key, value); err != nil {
						errors++
					} else {
						if _, _, err := client.Get(key); err != nil {
							errors++
						}
						client.Delete(key)
						ops++
					}
				}
			}
		}(i)
	}

	wg.Wait()
	actualDuration := time.Since(start)

	avgOps := float64(totalOps) / actualDuration.Seconds()
	errorRate := float64(totalErrors) / float64(totalOps+totalErrors) * 100

	fmt.Printf("     æ€»æ“ä½œæ•°: %d\n", totalOps)
	fmt.Printf("     æ€»é”™è¯¯æ•°: %d\n", totalErrors)
	fmt.Printf("     å¹³å‡QPS: %.2f ops/s\n", avgOps)
	fmt.Printf("     é”™è¯¯ç‡: %.2f%%\n", errorRate)
	fmt.Printf("     å®é™…è¿è¡Œæ—¶é—´: %v\n", actualDuration)

	if errorRate < 1.0 {
		fmt.Printf("     ğŸ‰ é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§: ä¼˜ç§€\n")
	} else if errorRate < 5.0 {
		fmt.Printf("     âœ… é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§: è‰¯å¥½\n")
	} else {
		fmt.Printf("     âš ï¸ é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§: éœ€è¦ä¼˜åŒ–\n")
	}
}
