package distributed

import (
	"bytes"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"sync"
	"time"
)

// ClusterManager é›†ç¾¤ç®¡ç†å™¨
// è´Ÿè´£èŠ‚ç‚¹å‘ç°ã€å¥åº·æ£€æŸ¥ã€æ•…éšœæ¢å¤ç­‰
type ClusterManager struct {
	nodeID       string
	nodes        map[string]*NodeInfo // nodeID -> NodeInfo
	mu           sync.RWMutex
	httpClient   *http.Client
	healthTicker *time.Ticker
	stopChan     chan struct{}
}

// NodeInfo èŠ‚ç‚¹ä¿¡æ¯
type NodeInfo struct {
	NodeID      string    `json:"node_id"`
	Address     string    `json:"address"`
	Status      string    `json:"status"`      // "healthy", "unhealthy", "unknown"
	LastSeen    time.Time `json:"last_seen"`
	ResponseTime int64    `json:"response_time"` // å“åº”æ—¶é—´(æ¯«ç§’)
}

// ClusterStatus é›†ç¾¤çŠ¶æ€
type ClusterStatus struct {
	TotalNodes    int         `json:"total_nodes"`
	HealthyNodes  int         `json:"healthy_nodes"`
	UnhealthyNodes int        `json:"unhealthy_nodes"`
	Nodes         []*NodeInfo `json:"nodes"`
	LastUpdate    time.Time   `json:"last_update"`
}

// NewClusterManager åˆ›å»ºé›†ç¾¤ç®¡ç†å™¨
func NewClusterManager(nodeID string, clusterNodes map[string]string) *ClusterManager {
	cm := &ClusterManager{
		nodeID: nodeID,
		nodes:  make(map[string]*NodeInfo),
		httpClient: &http.Client{
			Timeout: 3 * time.Second,
		},
		stopChan: make(chan struct{}),
	}
	
	// åˆå§‹åŒ–èŠ‚ç‚¹ä¿¡æ¯
	for id, addr := range clusterNodes {
		cm.nodes[id] = &NodeInfo{
			NodeID:   id,
			Address:  addr,
			Status:   "unknown",
			LastSeen: time.Now(),
		}
	}
	
	return cm
}

// Start å¯åŠ¨é›†ç¾¤ç®¡ç†å™¨
func (cm *ClusterManager) Start() error {
	log.Printf("ğŸŒ å¯åŠ¨é›†ç¾¤ç®¡ç†å™¨ï¼ŒèŠ‚ç‚¹ID: %s", cm.nodeID)
	
	// å¯åŠ¨å¥åº·æ£€æŸ¥
	cm.healthTicker = time.NewTicker(10 * time.Second)
	go cm.healthCheckLoop()
	
	// å‘å…¶ä»–èŠ‚ç‚¹å®£å‘Šè‡ªå·±çš„å­˜åœ¨
	go cm.announceToCluster()
	
	return nil
}

// Stop åœæ­¢é›†ç¾¤ç®¡ç†å™¨
func (cm *ClusterManager) Stop() {
	if cm.healthTicker != nil {
		cm.healthTicker.Stop()
	}
	
	close(cm.stopChan)
	log.Printf("ğŸ›‘ é›†ç¾¤ç®¡ç†å™¨å·²åœæ­¢")
}

// announceToCluster å‘é›†ç¾¤å®£å‘ŠèŠ‚ç‚¹åŠ å…¥
func (cm *ClusterManager) announceToCluster() {
	time.Sleep(2 * time.Second) // ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
	
	cm.mu.RLock()
	currentNode := cm.nodes[cm.nodeID]
	cm.mu.RUnlock()
	
	if currentNode == nil {
		return
	}
	
	joinData := map[string]string{
		"node_id": cm.nodeID,
		"address": currentNode.Address,
	}
	
	for nodeID, node := range cm.nodes {
		if nodeID != cm.nodeID {
			cm.notifyNodeJoin(node.Address, joinData)
		}
	}
}

// notifyNodeJoin é€šçŸ¥å…¶ä»–èŠ‚ç‚¹æœ‰æ–°èŠ‚ç‚¹åŠ å…¥
func (cm *ClusterManager) notifyNodeJoin(targetAddr string, joinData map[string]string) {
	jsonData, _ := json.Marshal(joinData)
	url := fmt.Sprintf("http://%s/internal/cluster/join", targetAddr)
	
	resp, err := cm.httpClient.Post(url, "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		log.Printf("âš ï¸ é€šçŸ¥èŠ‚ç‚¹åŠ å…¥å¤±è´¥ %s: %v", targetAddr, err)
		return
	}
	defer resp.Body.Close()
	
	if resp.StatusCode == http.StatusOK {
		log.Printf("âœ… æˆåŠŸé€šçŸ¥èŠ‚ç‚¹ %s å…³äºèŠ‚ç‚¹åŠ å…¥", targetAddr)
	}
}

// healthCheckLoop å¥åº·æ£€æŸ¥å¾ªç¯
func (cm *ClusterManager) healthCheckLoop() {
	for {
		select {
		case <-cm.healthTicker.C:
			cm.performHealthCheck()
		case <-cm.stopChan:
			return
		}
	}
}

// performHealthCheck æ‰§è¡Œå¥åº·æ£€æŸ¥
func (cm *ClusterManager) performHealthCheck() {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	
	for nodeID, node := range cm.nodes {
		if nodeID == cm.nodeID {
			// è‡ªå·±æ€»æ˜¯å¥åº·çš„
			node.Status = "healthy"
			node.LastSeen = time.Now()
			continue
		}
		
		// æ£€æŸ¥å…¶ä»–èŠ‚ç‚¹
		start := time.Now()
		healthy := cm.checkNodeHealth(node.Address)
		responseTime := time.Since(start).Milliseconds()
		
		if healthy {
			node.Status = "healthy"
			node.LastSeen = time.Now()
			node.ResponseTime = responseTime
		} else {
			node.Status = "unhealthy"
			node.ResponseTime = -1
		}
	}
}

// checkNodeHealth æ£€æŸ¥å•ä¸ªèŠ‚ç‚¹å¥åº·çŠ¶æ€
func (cm *ClusterManager) checkNodeHealth(address string) bool {
	url := fmt.Sprintf("http://%s/internal/cluster/health", address)
	
	resp, err := cm.httpClient.Get(url)
	if err != nil {
		return false
	}
	defer resp.Body.Close()
	
	return resp.StatusCode == http.StatusOK
}

// AddNode æ·»åŠ èŠ‚ç‚¹åˆ°é›†ç¾¤
func (cm *ClusterManager) AddNode(nodeID, address string) {
	cm.mu.Lock()
	defer cm.mu.Unlock()

	cm.nodes[nodeID] = &NodeInfo{
		NodeID:   nodeID,
		Address:  address,
		Status:   "unknown",
		LastSeen: time.Now(),
	}
	
	log.Printf("â• æ·»åŠ èŠ‚ç‚¹åˆ°é›†ç¾¤: %s (%s)", nodeID, address)
}

// RemoveNode ä»é›†ç¾¤ä¸­ç§»é™¤èŠ‚ç‚¹
func (cm *ClusterManager) RemoveNode(nodeID string) {
	cm.mu.Lock()
	defer cm.mu.Unlock()
	
	delete(cm.nodes, nodeID)
	log.Printf("â– ä»é›†ç¾¤ä¸­ç§»é™¤èŠ‚ç‚¹: %s", nodeID)
}

// GetNodes è·å–æ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯
func (cm *ClusterManager) GetNodes() map[string]*NodeInfo {
	cm.mu.RLock()
	defer cm.mu.RUnlock()
	
	// è¿”å›å‰¯æœ¬
	nodes := make(map[string]*NodeInfo)
	for id, node := range cm.nodes {
		nodeCopy := *node
		nodes[id] = &nodeCopy
	}
	
	return nodes
}

// GetHealthyNodes è·å–å¥åº·çš„èŠ‚ç‚¹
func (cm *ClusterManager) GetHealthyNodes() map[string]*NodeInfo {
	cm.mu.RLock()
	defer cm.mu.RUnlock()
	
	healthyNodes := make(map[string]*NodeInfo)
	for id, node := range cm.nodes {
		if node.Status == "healthy" {
			nodeCopy := *node
			healthyNodes[id] = &nodeCopy
		}
	}
	
	return healthyNodes
}

// GetClusterStatus è·å–é›†ç¾¤çŠ¶æ€
func (cm *ClusterManager) GetClusterStatus() *ClusterStatus {
	cm.mu.RLock()
	defer cm.mu.RUnlock()
	
	status := &ClusterStatus{
		TotalNodes:   len(cm.nodes),
		LastUpdate:   time.Now(),
		Nodes:        make([]*NodeInfo, 0, len(cm.nodes)),
	}
	
	for _, node := range cm.nodes {
		nodeCopy := *node
		status.Nodes = append(status.Nodes, &nodeCopy)
		
		if node.Status == "healthy" {
			status.HealthyNodes++
		} else {
			status.UnhealthyNodes++
		}
	}
	
	return status
}

// IsHealthy æ£€æŸ¥é›†ç¾¤æ˜¯å¦å¥åº·
func (cm *ClusterManager) IsHealthy() bool {
	cm.mu.RLock()
	defer cm.mu.RUnlock()
	
	healthyCount := 0
	for _, node := range cm.nodes {
		if node.Status == "healthy" {
			healthyCount++
		}
	}
	
	// è‡³å°‘ä¸€åŠèŠ‚ç‚¹å¥åº·æ‰è®¤ä¸ºé›†ç¾¤å¥åº·
	return healthyCount >= len(cm.nodes)/2
}

// Leave ç¦»å¼€é›†ç¾¤
func (cm *ClusterManager) Leave() error {
	leaveData := map[string]string{
		"node_id": cm.nodeID,
	}
	
	cm.mu.RLock()
	nodes := make([]*NodeInfo, 0, len(cm.nodes))
	for _, node := range cm.nodes {
		if node.NodeID != cm.nodeID {
			nodes = append(nodes, node)
		}
	}
	cm.mu.RUnlock()
	
	// é€šçŸ¥å…¶ä»–èŠ‚ç‚¹è‡ªå·±è¦ç¦»å¼€
	for _, node := range nodes {
		cm.notifyNodeLeave(node.Address, leaveData)
	}
	
	return nil
}

// notifyNodeLeave é€šçŸ¥å…¶ä»–èŠ‚ç‚¹æœ‰èŠ‚ç‚¹ç¦»å¼€
func (cm *ClusterManager) notifyNodeLeave(targetAddr string, leaveData map[string]string) {
	jsonData, _ := json.Marshal(leaveData)
	url := fmt.Sprintf("http://%s/internal/cluster/leave", targetAddr)
	
	resp, err := cm.httpClient.Post(url, "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		log.Printf("âš ï¸ é€šçŸ¥èŠ‚ç‚¹ç¦»å¼€å¤±è´¥ %s: %v", targetAddr, err)
		return
	}
	defer resp.Body.Close()
	
	if resp.StatusCode == http.StatusOK {
		log.Printf("âœ… æˆåŠŸé€šçŸ¥èŠ‚ç‚¹ %s å…³äºèŠ‚ç‚¹ç¦»å¼€", targetAddr)
	}
}
